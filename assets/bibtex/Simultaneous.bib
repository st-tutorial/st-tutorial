@inproceedings{oda-etal-2014-optimizing,
    title = "Optimizing Segmentation Strategies for Simultaneous Speech Translation",
    author = "Oda, Yusuke  and
      Neubig, Graham  and
      Sakti, Sakriani  and
      Toda, Tomoki  and
      Nakamura, Satoshi",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-2090",
    doi = "10.3115/v1/P14-2090",
    pages = "551--556",
}
@inproceedings{Niehues_Nguyen_Cho_Ha_Kilgour_Müller_Sperber_Stüker_Waibel_2016,
place={San Francisco, California, USA},
title={Dynamic Transcription for Low-latency Speech Translation},
url={http://isl.anthropomatik.kit.edu/pdf/Niehues2016.pdf},
booktitle={Proceedings of the 17th Annual Conference of the International Speech Communication Association (Interspeech 2016)},
author={Niehues, Jan and Nguyen, Thai-Son and Cho, Eunah and Ha, Thanh-Le and Kilgour, Kevin and Müller, Markus and Sperber, Matthias and Stüker, Sebastian and Waibel, Alex},
year={2016},
pages={2513–2517}
}

@inproceedings{Niehues_Pham_Ha_Sperber_Waibel_2018,
place={Hyderabad, India},
title={Low-Latency Neural Speech Translation},
url={https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1055.pdf},
booktitle={Proceedings of the 19th Annual Conference of the International Speech Communication Association (Interspeech 2018)},
author={Niehues, Jan and Pham, Ngoc-Quan and Ha, Thanh-Le and Sperber, Matthias and Waibel, Alex},
year={2018} }

@article{Arivazhagan2020ReTranslationSF,
  title={Re-Translation Strategies for Long Form, Simultaneous, Spoken Language Translation},
  author={N. Arivazhagan and Colin Cherry and I Te and Wolfgang Macherey and Pallavi Baljekar and G. Foster},
  journal={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
  pages={7919-7923}
}

@misc{cho2016neural,
      title={Can neural machine translation do simultaneous translation?},
      author={Kyunghyun Cho and Masha Esipova},
      year={2016},
      eprint={1606.02012},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{dalvi-etal-2018-incremental,
    title = "Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation",
    author = "Dalvi, Fahim  and
      Durrani, Nadir  and
      Sajjad, Hassan  and
      Vogel, Stephan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2079",
    doi = "10.18653/v1/N18-2079",
    pages = "493--499",
    abstract = "We address the problem of simultaneous translation by modifying the Neural MT decoder to operate with dynamically built encoder and attention. We propose a tunable agent which decides the best segmentation strategy for a user-defined BLEU loss and Average Proportion (AP) constraint. Our agent outperforms previously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova, 2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to Neural MT training to better match the incremental decoding framework.",
}
@inproceedings{gu-etal-2017-learning,
    title = "Learning to Translate in Real-time with Neural Machine Translation",
    author = "Gu, Jiatao  and
      Neubig, Graham  and
      Cho, Kyunghyun  and
      Li, Victor O.K.",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-1099",
    pages = "1053--1062",
    abstract = "Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.",
}
@inproceedings{zheng-etal-2019-simpler,
    title = "Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation",
    author = "Zheng, Baigong  and
      Zheng, Renjie  and
      Ma, Mingbo  and
      Huang, Liang",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1137",
    doi = "10.18653/v1/D19-1137",
    pages = "1349--1354",
    abstract = "Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories: (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ/WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German{\textless}={\textgreater}English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.",
}
@inproceedings{inproceedings,
author = {Kolss, Muntsin and Vogel, Stephan and Waibel, Alex},
year = {2008},
month = {01},
pages = {2735-2738},
title = {Stream decoding for simultaneous spoken language translation.}
}

@inproceedings{ren-etal-2020-simulspeech,
    title = "{S}imul{S}peech: End-to-End Simultaneous Speech to Text Translation",
    author = "Ren, Yi  and
      Liu, Jinglin  and
      Tan, Xu  and
      Zhang, Chen  and
      Qin, Tao  and
      Zhao, Zhou  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.350",
    doi = "10.18653/v1/2020.acl-main.350",
    pages = "3787--3796",
    abstract = "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-$k$ strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",
}

×
@inproceedings{Weller2021,
 address = {Kyiv, Ukraine},
 author = {Weller, Orion and Sperber, Matthias and Gollan, Christian, and Kluivers, Joris},
 booktitle = {European Chapter
of the Association for Computational Linguistic (EACL)},
 title = {{Streaming Models for Joint Speech Recognition and Translation}},
 year = {2021}
}

@inproceedings{Liu2020,
  author={Danni Liu and Gerasimos Spanakis and Jan Niehues},
  title={{Low-Latency Sequence-to-Sequence Speech Recognition and Translation by Partial Hypothesis Selection}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={3620--3624},
  doi={10.21437/Interspeech.2020-2897},
  url={http://dx.doi.org/10.21437/Interspeech.2020-2897}
}

@inproceedings{yao-haddow-2020-dynamic,
    title = "Dynamic Masking for Improved Stability in Online Spoken Language Translation",
    author = "Yao, Yuekun  and
      Haddow, Barry",
    booktitle = "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://www.aclweb.org/anthology/2020.amta-research.12",
    pages = "123--136",
}
@misc{ma2020streaming,
      title={Streaming Simultaneous Speech Translation with Augmented Memory Transformer},
      author={Xutai Ma and Yongqiang Wang and Mohammad Javad Dousti and Philipp Koehn and Juan Pino},
      year={2020},
      eprint={2011.00033},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
